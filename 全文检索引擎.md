全文检索引擎


分词

我们需要将原始文本分解成适合索引和搜索的单词（tokens）列表。
文本解析器由一个分词器和多个过滤器组成。


分词模式 使用搜索引擎模式
结巴分词有好几种模式
去除标点符号 

分词是文本解析的第一步，它的工作是将文本转换成一个单词列表。我们本次的实现是在一个词的边界上分割文本，并删除标点符号。
数字  停顿词  单字
排除常用词
词干化  小写话

倒排索引

回到倒排索引，它把文档中的每个词都映射到文档 ID 上。内置的 map 是存储该映射很好的选择。map 中的键为单词（字符串），值为文档 ID 的列表。

首先需要用输入数据创建索引，对于互联网搜索引擎，输入数据是一个个由爬虫从网上抓回来的网页，经过清洗之后进行内容抽取，然后整理成统一的格式交给索引程序创建索引。 索引由以下几个基本的组成部分： 1. 倒排索引，这一部分存放"关键字"->文档的映射，一般来说会把同一个关键字对应的所有文档按照统一方法整理成一个排好序的线性结构，以便遍历和各种AND/OR之类的操作。 2. 正排索引，这一部分存放每个文档的各种属性 索引程序要干的事就是从源数据中拿出每个关键字和各种属性，整理成索引文件。文本变成关键字的过程叫做关键字提取，对于英语等语言，这个过程相对容易，一般就是进行大小写、全角/半角转换，拼写检查，字根提取等工作，例如源文本中的“goes”，“going”，“went”统一转换为“go”等；对于中文来说这个过程会比较麻烦，需要进行分词，常用的分词方法有单字切分、正/反向最大匹配、n元分词、隐马模型等。没有那一种单一的方法能满足所有需求，所以实际应用中一般会将多种方法结合使用。


过滤器
大部数情况下，仅仅将文本转换为一个单词列表是不够的。为了使文本更容易被索引和搜索，我们还需要做额外的规范化处理。
小写字母
为了使搜索不区分大小写，小写过滤器将单词转换为小写。cAt、Cat 和 caT 被归一化为 cat。之后在我们查询索引时，也会将搜索词进行小写处理。这样就可以让搜索词 cAt 与文本 Cat 相匹配了。
排除常用词
几乎所有英语文本都包含常用的单词，如 a、I、the 或 be。这样的词被称为停词，我们要将它们删掉，因为几乎任何文档都会与这些停顿词相匹配。
没有「官方」的停词表，这里我们把 OEC 排名的前10的词进行排除。
词干化
由于语法规则的原因，文档中可能包括同一个词的不同形式。词干化将单词还原为其基本形式。例如，fishing、fished 和 fishe r可以被还原为基本形式（词干）fish。
实现词干化是一项很大的任务，在本文中不进行涉及。我们将采用现有的一个模块


搜索  关键词分库分区
多关键词搜索


布尔查询
上边的查询为每一个单词都返回了一个文档 ID 列表。当我们在搜索框中输入 small wild cat 时，我们通常期望找到的是一个同时包含 small、wild 和 cat 的结果列表。下一步是计算这些列表之间的集合交集，这样我们就可以得到一个与所有单词相匹配的文件列表。


我没有触及到太多可以显著提高性能和使引擎更友好的内容，下面是几个进一步改进的想法：

扩展布尔查询，支持 OR 和NOT

布尔查询
上边的查询为每一个单词都返回了一个文档 ID 列表。当我们在搜索框中输入 small wild cat 时，我们通常期望找到的是一个同时包含 small、wild 和 cat 的结果列表。下一步是计算这些列表之间的集合交集，这样我们就可以得到一个与所有单词相匹配的文件列表。


Pp00..ppP
